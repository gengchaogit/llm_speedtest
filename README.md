# 🚀 本地大模型推理速度测试工具 v1.9 (chao魔改版)

![Version](https://img.shields.io/badge/version-1.9-blue.svg)

---

🇬🇧 [**View in English (查看英文版)**](README_en.md) | 🇨🇳 [**查看中文版**](#)

---

## 简介

本项目是一个功能强大的浏览器端本地大模型推理性能测试工具，旨在帮助用户快速、便捷地测试本地部署的各种大模型推理服务的 Prefill（预填充）和 Decode（解码/输出）性能。它通过浏览器本地离线运行，无需服务器端部署，确保了数据的隐私性和测试的便捷性。

本工具基于原作者纸鸢随风（B站） 的作品进行魔改，增加了多项实用功能和优化，包括重试机制、图表自动显示、Prefill 和 Decode 速度分离、并发问题修复、请求超时控制以及配置自动保存等。

## 更新日志

<!-- 请注意，以下更新日志从v1.7开始列出。 -->

### v1.9 (Chao 魔改增强版 - 历史记录与详情查看)
*   **📚 历史记录自动保存**：测试完成后自动保存到浏览器 localStorage（最多保存 20 条）
*   **🗂️ 历史记录管理面板**：全新的历史记录模态框，美观易用
    *   测试时间、备注、配置、性能统计一目了然
    *   支持查看完整测试详情（配置、统计数据、完整表格）
    *   单个历史记录导出为 CSV
    *   删除单条记录或清空全部
*   **📊 历史记录对比功能**：勾选多条历史记录，一键生成对比图表
    *   复选框快速选择多个测试记录
    *   实时显示已选择数量
    *   自动滚动到对比图表位置
    *   与 CSV 导入功能完全兼容
*   **🔍 测试详情查看**：每个测试结果行都有"查看详情"按钮
    *   查看完整的提示词内容和模型输出
    *   显示详细的请求配置（Temperature、Top P 等）
    *   展示完整的性能指标（Prefill/Decode 时间和速度）
    *   区分思考内容和答案内容（支持推理模型）
    *   支持一键复制提示词和输出内容
    *   美观的模态框界面，内容自动换行
*   **💾 数据持久化**：所有历史记录保存在 localStorage，无需担心数据丢失
*   **🎨 完整双语支持**：所有新增功能完全支持中英文切换

### v1.8 (Chao 魔改增强版)
*   **🌐 双语支持（中英文）**：新增完整的中英文双语支持，配备醒目的语言切换按钮。界面可在中英文之间自动切换，所选语言自动保存到localStorage。
*   **📁 双语言文件版本**：
    *   `本地大模型推理速度测试工具v1.9.html` - 中文版（默认中文界面）
    *   `LLM_Speed_Test_Tool_v1.9_EN.html` - 英文版（默认英文界面）
*   **图表导出按钮**：添加一键导出按钮，方便将当前 Prefill 和 Decode 图表导出为一张图片。
*   **多个CSV结果导入对比**：新增功能，允许导入多个历史测试的 CSV 文件，在图表中进行对比分析。
*   **合并图表导出**：将 Prefill 和 Decode 图表一键导出为一张图片，方便分享和记录。
*   **对比标签**：导入 CSV 文件后，使用 CSV 文件名作为对比图表的图例标签，更易于区分不同测试结果。
*   **对比导出**：为对比图表也增加一键导出功能，并自动生成包含参数的文件名，便于管理。
*   **【重要修复】支持思考模型**：解析 usage 字段获取准确 token 统计（reasoning_tokens + completion_tokens）。
*   **添加token估算fallback**：当API不返回usage时使用字符估算。
*   **【关键修复】Prefill速度计算错误**：现在使用API返回的实际prompt_tokens而非估算值（修复了约10%误差）。
*   **【兼容性修复】支持多种流式响应格式**：delta.reasoning_content（思考）/ delta.content / message.content / text。
*   **自动保存/恢复所有配置参数**：API地址、模型、温度、并发数等所有参数，下次打开自动恢复。
*   **【思考模型修复】正确计算reasoning+completion总tokens**：当usage不准时使用内容估算，确保decode速度准确。
*   **【性能测量优化】优先使用服务器返回的真实GPU处理时间**：(prompt_eval_duration/eval_duration），消除网络延迟影响。
*   **【前缀缓存修复】并发测试使用唯一提示词**：每个并发请求现在都获得唯一的提示词前缀，防止vLLM的前缀缓存虚增性能指标。

### v1.7 (chao魔改版)

*   为每个请求增加 3 次重试（间隔 1.5 秒），提高测试稳定性，过滤瞬时失败。
*   自动显示图表，无需点击，提高使用便捷性。
*   将 Prefill 和 Decode 速度分离到两张图表，更清晰地展示性能。
*   修复高并发下 `Promise.all` 熔断问题，改用 `Promise.allSettled`，确保部分请求失败不影响整体并发流程。
*   增加 `AbortController` 实现的请求超时。
*   保存当前配置到 `localStorage`，下次打开自动加载。
*   优化了 UI 样式和用户体验。


## 特性

*   **🌐 完整双语支持**：完整的中英文界面，支持即时语言切换。语言偏好自动保存。
*   **浏览器本地运行**：完全离线，数据不出本地，保障隐私安全。
*   **支持多种 API 类型**：
    *   **OpenAI 兼容接口**：适用于所有兼容 OpenAI API 的大模型服务，如 vLLM、TGI、FastChat 等。
    *   **Ollama 接口**：直接利用 Ollama 提供的性能指标，简化测试。
*   **详细性能指标**：测量提示词长度、预填充耗时、预填充速度、输出 Token 数、输出耗时、输出速度，包括 P50/P90/P95 百分位统计。
*   **并发测试**：支持设置并发请求数，模拟高负载场景。
*   **请求重试机制**：每个请求增加 3 次重试（间隔 1.5 秒），有效过滤瞬时/偶发性网络或服务器失败。
*   **请求超时控制**：通过 AbortController 实现请求超时，避免长时间无响应。
*   **实时进度条与结果显示**：测试进度可视化，结果实时更新。
*   **自动图表生成**：测试过程中自动生成并更新 Prefill 和 Decode 吞吐量图表，直观展示性能趋势。
*   **📚 历史记录管理**（v1.9 新增）：
    *   **自动保存**：测试完成后自动保存到 localStorage（最多 20 条）
    *   **历史面板**：美观的模态框界面，查看所有历史测试
    *   **详情查看**：查看历史记录的完整配置、统计数据和测试结果
    *   **单个导出**：将历史记录导出为 CSV 文件
    *   **记录管理**：删除单条记录或清空全部
    *   **对比功能**：勾选多条历史记录，一键生成对比图表
*   **🔍 测试详情查看**（v1.9 新增）：
    *   **详情按钮**：每个测试结果行都有"查看详情"按钮
    *   **完整内容**：查看完整的提示词和模型输出内容
    *   **请求配置**：显示所有请求参数（API类型、模型、Temperature、Top P等）
    *   **性能指标**：展示详细的Prefill/Decode时间和速度
    *   **推理模型支持**：区分思考内容和答案内容
    *   **复制功能**：一键复制提示词或输出内容到剪贴板
    *   **优化排版**：文本自动换行，表格列宽优化，无需横向滚动
*   **配置自动保存**：自动将当前的 API 地址、模型名称和 API 类型保存到浏览器 `localStorage`，下次打开自动加载。
*   **结果导出**：支持将测试结果复制为 Markdown 表格或导出为 CSV 文件。
*   **图表导出**：支持将生成的 Prefill 和 Decode 吞吐图表一键导出为图片。
*   **CSV结果导入与对比**：支持导入多个历史CSV测试结果，并在图表中进行对比分析。
*   **灵活的参数配置**：可自定义提示词长度范围、步长、输出长度、Temperature、Top P、Penalty 等。

## 核心概念

*   **OpenAI 兼容接口**：指符合 OpenAI `v1/chat/completions` 标准的大模型推理 API。本工具通过发送 `stream=true` 的请求，测量首字到达时间来计算 Prefill 速度，以及总输出完成时间来计算 Decode 速度。
*   **Ollama 接口**：指 Ollama 提供的 `api/chat` 接口。本工具通过发送 `stream=false` 的请求，直接利用 Ollama 响应中返回的 `prompt_eval_duration` 和 `eval_duration` 等指标来计算性能。
*   **Prefill (预填充) 吞吐**：衡量模型处理输入提示词的速度，通常以 **tokens/秒** 为单位。对于流式输出，从请求开始到第一个 token 到达的时间用于计算。
*   **Decode (解码/输出) 吞吐**：衡量模型生成输出 token 的速度，通常以 **tokens/秒** 为单位。对于流式输出，从第一个 token 到达直到所有输出完成的时间用于计算。

## 如何使用

### 1. 本地运行

1.  **选择你偏好的版本**：
    *   默认英文界面：下载 `LLM_Speed_Test_Tool_v1.9_EN.html`
    *   默认中文界面：下载 `本地大模型推理速度测试工具v1.9.html`
2.  使用任何现代浏览器（如 Chrome, Firefox, Edge）打开HTML文件。无需额外的安装或服务器。
3.  **语言切换**：点击右上角的语言切换按钮（紫色渐变）可随时在中英文之间切换。你的选择会自动保存。

### 2. 配置参数

在页面中填写或选择以下关键参数：

*   **API 类型选择**：选择 `OpenAI兼容接口` 或 `Ollama接口`。
*   **API 地址**：你的大模型推理服务 API 的完整 URL。
    *   OpenAI 兼容接口: 通常为 `http://your_ip:port/v1/chat/completions`
    *   Ollama 接口: 通常为 `http://your_ip:port/api/chat`
*   **模型名称**：你在 API 中使用的模型名称或 ID。
*   **API-Key**：如果你的 API 需要认证，请填写。不需要则留空。
*   **备注**：填写设备信息、模型信息、模型推理框架等，方便后续回顾。
*   **最小提示词长度**：测试的起始提示词 token 数量。
*   **最大提示词长度**：测试的结束提示词 token 数量。
*   **步长**：每次测试提示词长度增加的量。
*   **期望输出长度**：每次模型应生成的最大 token 数量。
*   **并发数**：同时发送的请求数量，用于测试并发性能。
*   **请求超时 (ms)**：单个请求的最大等待时间（毫秒）。
*   **Temperature / Top P / Presence Penalty / Frequency Penalty**：生成参数，用于控制模型输出的随机性和多样性。

### 3. 启动测试

点击 `开始测试` 按钮。测试将按照你设定的提示词长度范围和步长，逐次发送请求。

### 4. 停止测试

点击 `停止测试` 按钮（测试开始后显示），可随时中断正在进行的测试。

### 5. 查看结果

测试结果将实时显示在页面下方的表格中，包括：
*   **提示词长度 (tokens)**
*   **预填充耗时 (ms)**
*   **预填充速度 (tokens/s)**
*   **输出长度 (tokens)**
*   **输出耗时 (ms)**
*   **输出速度 (tokens/s)**
*   **状态** (成功/失败)
*   **并发统计**：测试完成后，将显示总体 Prefill 和 Decode 吞吐的最小、最大和平均值。

同时，`预填充 (Prefill) 吞吐` 和 `输出 (Decode) 吞吐` 两张图表也会自动生成并实时更新，直观展示性能随提示词长度的变化趋势。

### 6. 复制 Markdown 表格

测试完成后，点击 `复制 Markdown 表格` 按钮，可以将测试数据以 Markdown 表格形式复制到剪贴板，方便粘贴到文档或 GitHub README 中。

### 7. 导出 CSV 数据

测试完成后，点击 `导出 CSV 数据` 按钮，可以将测试数据导出为 CSV 文件，方便在 Excel 或其他数据分析工具中进行进一步分析。
为了方便进行版本对比，建议每次导出CSV时在文件名中带上当前的版本号或者测试时间。

### 8. 导出图表

测试完成后，点击图表区域上方的 `导出图表` 按钮，可将 Prefill 和 Decode 吞吐量图表合并导出为一张 PNG 图片。

### 9. 导入 CSV 数据进行对比

点击 `导入CSV文件` 按钮，选择一个或多个之前导出的 CSV 文件。程序将解析这些文件的数据，并在 Prefill 和 Decode 吞吐量图表中绘制多条曲线，方便你对比不同模型、不同配置或不同版本的性能。导入的 CSV 文件名将作为图例标签。

### 10. 导出对比图表

导入 CSV 文件并绘制对比图表后，点击图表上方的 `导出对比图表` 按钮，即可将包含所有对比数据的 Prefill 和 Decode 吞吐量图表合并导出为一张 PNG 图片，文件名会自动包含当前测试参数和导入的文件名，便于管理。

### 11. 查看历史记录（v1.9 新增）

点击 `📚 查看历史记录` 按钮，打开历史记录管理面板。历史记录会在每次测试完成后自动保存（最多保存 20 条）。

在历史记录面板中，你可以：
*   **查看列表**：浏览所有历史测试的摘要信息（时间、备注、模型、配置、性能统计）
*   **查看详情**：点击 `查看详情` 按钮，查看某条记录的完整配置、统计数据和测试结果表格
*   **导出 CSV**：点击 `导出CSV` 按钮，将该历史记录导出为 CSV 文件
*   **删除记录**：点击 `删除` 按钮，删除不需要的历史记录
*   **清空全部**：点击 `清空全部` 按钮，一键清除所有历史记录

### 12. 历史记录对比（v1.9 新增）

在历史记录面板中，你可以勾选多条历史记录进行对比：

1.  在历史记录列表中，勾选你想对比的记录（左侧复选框）
2.  顶部会实时显示 "已选择 X 条"
3.  点击 `生成对比图表` 按钮
4.  历史记录面板自动关闭，页面滚动到对比图表区域
5.  查看多条历史记录的 Prefill 和 Decode 性能对比曲线

**提示**：历史记录对比功能与 CSV 导入功能完全兼容，你可以同时使用两种方式进行对比分析。

### 13. 查看测试详情（v1.9 新增）

在测试结果表格中，每一行测试结果都有一个 `查看详情` 按钮。点击该按钮可以打开详情弹窗，查看该测试的完整信息：

详情弹窗包含以下内容：
*   **基本信息**：API类型、模型名称、提示词长度、输出长度、并发数、测试时间
*   **性能指标**：预填充耗时/速度、输出耗时/速度、测试状态
*   **请求配置**：Temperature、Top P、Max Tokens 等所有请求参数
*   **提示词内容**：完整的提示词文本，支持一键复制
*   **输出内容**：完整的模型输出文本，支持一键复制
    *   对于推理模型（如 DeepSeek-R1），会区分显示思考内容和答案内容
    *   所有文本自动换行，方便阅读

**使用技巧**：
*   点击详情弹窗外的遮罩层或"关闭"按钮可关闭弹窗
*   点击"复制内容"按钮可快速复制提示词或输出内容
*   详情弹窗内容可滚动查看，适合查看长文本

## 配置说明

以下是各配置项的详细说明：

*   **API 地址 (`apiUrl`)**：
    *   **OpenAI 兼容**：例如 `http://192.168.1.100:8000/v1/chat/completions`。请确保你的大模型推理框架（如 vLLM, TGI）以兼容 OpenAI API 的模式运行。
    *   **Ollama**：例如 `http://localhost:11434/api/chat`。确保 Ollama 服务正在运行。
*   **模型名称 (`modelName`)**：
    *   **OpenAI 兼容**：你的服务所使用的模型 ID 或名称，例如 `qwen-7b-chat`，`mistral-7b-instruct`。
    *   **Ollama**：你在 Ollama 中拉取的模型名称，例如 `llama2`，`qwen`。
*   **API-Key (`apiKey`)**：如果你的大模型服务设置了 API Key 认证，请在此处填写。否则可以留空。
*   **备注 (`notes`)：** 用于记录每次测试的环境信息，例如 `GeForce RTX 4090, vLLM 0.3.0, Qwen-7B-Chat-AWQ`。
*   **最小/最大提示词长度 (`minLength`, `maxLength`)**：定义测试的提示词 token 长度区间。
*   **步长 (`step`)**：每次测试时提示词长度递增的数量。例如，`minLength=128, maxLength=1024, step=128` 会测试 128, 256, 384...1024 token 的提示词。
*   **期望输出长度 (`outputLength`)**：模型每次请求预计生成的 token 数量。
*   **并发数 (`concurrency`)**：同时发送的请求数。例如设置为 `4`，则每个提示词长度会同时进行 `4` 次请求，并计算它们的平均性能及总吞吐。
*   **请求超时 (ms) (`timeout`)**：单个请求的最大等待时间（毫秒）。如果模型在此时间内未能完成响应，请求将被视为失败。
*   **Temperature (`temperatureInput`)**：控制模型输出的随机性。数值越高，输出越随机。
*   **Top P (`topPInput`)**：控制模型从高概率到低概率选择词汇的范围。例如，`0.9` 意味着模型将选择累积概率达到 90% 的词汇。
*   **Presence Penalty (`presencePenaltyInput`) / Frequency Penalty (`frequencyPenaltyInput`)**：用于减少重复性。Presence Penalty 惩罚已出现过的 token，Frequency Penalty 惩罚出现频率高的 token。Ollama 接口通常合并为 `repeat_penalty`。

## 故障排除

*   **测试长时间无响应/失败**：
    *   检查 `API 地址` 是否正确、网络是否可达。
    *   检查 `模型名称` 是否与服务端配置一致。
    *   检查 `API-Key` 是否正确。
    *   增大 `请求超时 (ms)`。
    *   对于 OpenAI 兼容接口，确保你的服务支持流式（`stream: true`）响应。
*   **Ollama 接口测试失败，提示缺少 `duration` 字段**：
    *   确保 Ollama 服务版本较新，并返回必要的性能指标。
*   **图表不显示或数据异常**：
    *   确认浏览器控制台（F12）是否有错误信息。
    *   清理浏览器缓存后重试。
*   **并发测试结果差异大**：
    *   确保你的硬件资源（GPU 显存、CPU 线程）能够支撑高并发。
    *   后台可能存在其他负载干扰测试结果。
    *   模型的服务配置（如最大 batch size）可能会影响高并发下的表现。
*   **CSV 导入图表不显示数据或显示错误**：
    *   请确保导入的 CSV 文件是本工具导出的标准格式。
    *   检查 CSV 文件中的列名是否与程序预期一致（如 "提示词长度 (tokens)", "预填充速度 (tokens/s)", "输出速度 (tokens/s)"）。

## 贡献与反馈

欢迎提交 issues 或 pull requests 来改进本工具或报告 bug。

---

**原作者：纸鸢随风（B站）、DeepSeek-R1-0528**
**基于原始作者的最新版改动，可查看**
**https://www.bilibili.com/opus/1078272739661316119**

**魔改版维护者：chao (QQ群: 1028429001)**
**最新版获取地址：https://github.com/gengchaogit/llm_speedtest**
